# Service Ollama pour DocuCortex

## Vue d'ensemble

Le service Ollama permet l'intÃ©gration d'un modÃ¨le IA local (Llama 3.2 3B) dans DocuCortex pour offrir des fonctionnalitÃ©s avancÃ©es d'analyse de texte, chat intelligent, et traitement de documents.

## FonctionnalitÃ©s

### ğŸ¤– Chat Intelligent
- Conversation contextuelle avec Ollama
- Support multi-modÃ¨les
- IntÃ©gration avec la recherche vectorielle
- Historique de conversation

### ğŸ“Š Analyse de Texte
- **Sentiment Analysis** : Analyse du sentiment des textes
- **RÃ©sumÃ© Intelligent** : RÃ©sumÃ©s automatiques de documents
- **Extraction de Mots-clÃ©s** : Identification automatique des concepts importants
- **Traduction** : Support multilingue

### â“ Q&A sur Documents
- Questions-rÃ©ponses sur les documents indexÃ©s
- Contexte intelligent basÃ© sur le contenu
- RÃ©ponses avec citations et rÃ©fÃ©rences

## Installation

### 1. Installation d'Ollama

```bash
# Installer Ollama (Linux/macOS)
curl -fsSL https://ollama.ai/install.sh | sh

# Sur Windows, tÃ©lÃ©charger depuis https://ollama.ai

# VÃ©rifier l'installation
ollama --version
```

### 2. Installation du modÃ¨le Llama 3.2 3B

```bash
# TÃ©lÃ©charger le modÃ¨le
ollama pull llama3.2:3b

# VÃ©rifier les modÃ¨les disponibles
ollama list
```

### 3. Configuration DocuCortex

Le service s'intÃ¨gre automatiquement dans DocuCortex. Les endpoints suivants sont disponibles :

## API Endpoints

### Statut et Configuration

#### `GET /ai/ollama/status`
VÃ©rifie le statut du service Ollama et DocuCortex IA.

```json
{
  "success": true,
  "aiService": {
    "success": true,
    "provider": "ollama",
    "ollamaAvailable": true
  },
  "ollama": {
    "enabled": true,
    "provider": "ollama",
    "model": {
      "name": "llama3.2:3b",
      "available": true
    },
    "stats": {
      "totalRequests": 0,
      "successRate": "0%"
    }
  }
}
```

#### `GET /ai/ollama/models`
Liste les modÃ¨les Ollama disponibles.

#### `POST /ai/ollama/model`
Change le modÃ¨le actif.

```json
{
  "modelName": "llama3.2:3b"
}
```

### Chat et Conversation

#### `POST /ai/chat/enhanced`
Chat amÃ©liorÃ© avec support Ollama.

```json
{
  "message": "Bonjour, comment allez-vous ?",
  "sessionId": "session123",
  "userId": "user456",
  "aiProvider": "ollama" // ou "default"
}
```

#### `POST /ai/ollama/chat`
Chat direct avec Ollama (mode expert).

```json
{
  "message": "Explique-moi l'intelligence artificielle",
  "systemPrompt": "Tu es un expert en IA",
  "temperature": 0.7,
  "maxTokens": 512
}
```

### Analyse de Contenu

#### `POST /ai/ollama/sentiment`
Analyse le sentiment d'un texte.

```json
{
  "text": "Je suis trÃ¨s satisfait de ce produit"
}
```

#### `POST /ai/ollama/summarize`
GÃ©nÃ¨re un rÃ©sumÃ© d'un texte.

```json
{
  "text": "Texte Ã  rÃ©sumer...",
  "maxLength": 200
}
```

#### `POST /ai/ollama/keywords`
Extrait les mots-clÃ©s d'un texte.

```json
{
  "text": "Texte pour extraction...",
  "maxKeywords": 10
}
```

#### `POST /ai/ollama/translate`
Traduit un texte.

```json
{
  "text": "Hello, how are you?",
  "targetLanguage": "franÃ§ais"
}
```

### Questions-RÃ©ponses

#### `POST /ai/ollama/qa`
Pose une question sur un document.

```json
{
  "documentId": 123,
  "question": "Quel est le sujet principal de ce document ?"
}
```

### Statistiques et Monitoring

#### `GET /ai/ollama/stats`
Statistiques complÃ¨tes du service.

#### `GET /ai/ollama/test`
Test de connexion Ã  Ollama.

#### `POST /ai/ollama/reset-stats`
Remet Ã  zÃ©ro les statistiques.

## Configuration

### Variables d'Environnement

```bash
# Port Ollama (dÃ©faut: 11434)
OLLAMA_HOST=http://localhost:11434

# ModÃ¨le par dÃ©faut (dÃ©faut: llama3.2:3b)
OLLAMA_MODEL=llama3.2:3b
```

### ParamÃ¨tres de ModÃ¨le

Les paramÃ¨tres suivants peuvent Ãªtre ajustÃ©s :

- **temperature** : CrÃ©ativitÃ© des rÃ©ponses (0.1-1.0)
- **top_p** : DiversitÃ© du vocabulaire (0.1-1.0)
- **top_k** : Nombre de choix pour chaque token
- **maxTokens** : Longueur maximale des rÃ©ponses
- **stop** : SÃ©quences d'arrÃªt personnalisÃ©es

## Utilisation ProgrammatÃ©e

### JavaScript/Node.js

```javascript
const aiService = require('./backend/services/ai/aiService');

// Chat avec Ollama
const chatResult = await aiService.processQuery(
    'session123',
    'Explique-moi les rÃ©seaux de neurones',
    'user456',
    { aiProvider: 'ollama' }
);

// Analyse de sentiment
const sentiment = await aiService.analyzeSentiment(
    'Ce produit est excellent !'
);

// RÃ©sumÃ© de document
const summary = await aiService.summarizeText(
    documentContent,
    150 // 150 caractÃ¨res max
);

// Q&A sur document
const qaResult = await aiService.answerQuestion(
    documentId,
    'Quel est le prix mentionnÃ© dans ce document ?'
);
```

### cURL Examples

```bash
# Test de connexion
curl -X GET http://localhost:3000/api/ai/ollama/test

# Chat avec Ollama
curl -X POST http://localhost:3000/api/ai/ollama/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour, peux-tu m'aider avec DocuCortex ?",
    "temperature": 0.7
  }'

# Analyse de sentiment
curl -X POST http://localhost:3000/api/ai/ollama/sentiment \
  -H "Content-Type: application/json" \
  -d '{"text": "Je suis trÃ¨s content de ce service"}'

# Q&A sur document
curl -X POST http://localhost:3000/api/ai/ollama/qa \
  -H "Content-Type: application/json" \
  -d '{
    "documentId": 123,
    "question": "RÃ©sume ce document en 3 points"
  }'
```

## Monitoring et Debugging

### Logs

Le service gÃ©nÃ¨re des logs dÃ©taillÃ©s :

```
ğŸ¤– Service Ollama intÃ©grÃ© avec succÃ¨s
ğŸ¤– Traitement avec Ollama...
âœ… Connexion Ollama OK (245ms)
âœ… RÃ©ponse Ollama gÃ©nÃ©rÃ©e (1250ms, 45 tokens)
ğŸ’­ Analyse sentiment: positif (87%)
ğŸ“ RÃ©sumÃ© gÃ©nÃ©rÃ©: 75% de compression
```

### Statistiques Disponibles

- Nombre total de requÃªtes
- Taux de succÃ¨s
- Temps de rÃ©ponse moyen
- Nombre de tokens gÃ©nÃ©rÃ©s
- Statut de connexion
- ModÃ¨les disponibles

### DÃ©pannage

#### Ollama non disponible
```bash
# VÃ©rifier qu'Ollama fonctionne
curl http://localhost:11434/api/tags

# RedÃ©marrer Ollama
ollama serve &
```

#### ModÃ¨le non trouvÃ©
```bash
# TÃ©lÃ©charger le modÃ¨le requis
ollama pull llama3.2:3b

# VÃ©rifier la liste des modÃ¨les
ollama list
```

#### Erreurs de connexion
- VÃ©rifier que Ollama est dÃ©marrÃ© : `ollama serve`
- VÃ©rifier le port (dÃ©faut: 11434)
- VÃ©rifier les variables d'environnement

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   DocuCortex     â”‚    â”‚     Ollama      â”‚
â”‚   (React)       â”‚â—„â”€â”€â–ºâ”‚   Backend        â”‚â—„â”€â”€â–ºâ”‚   (Local AI)    â”‚
â”‚                 â”‚    â”‚   aiService.js   â”‚    â”‚                 â”‚
â”‚                 â”‚    â”‚   ollamaService  â”‚    â”‚  llama3.2:3b    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚   Vector Search  â”‚
         â”‚              â”‚   Database       â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   WebSocket     â”‚
â”‚   (Real-time)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Limitations

- **RAM** : Llama 3.2 3B nÃ©cessite ~2GB RAM
- **CPU** : Processeur multi-cÅ“urs recommandÃ©
- **Stockage** : ~2GB pour le modÃ¨le
- **Latence** : 1-5 secondes selon la complexitÃ©
- **Contexte** : LimitÃ© Ã  ~128k tokens

## SÃ©curitÃ©

- Communication locale uniquement
- Pas de donnÃ©es envoyÃ©es vers des services externes
- ModÃ¨les et donnÃ©es restent sur la machine locale
- Validation des entrÃ©es utilisateur
- Gestion d'erreurs robuste

## Contribution

Pour contribuer au service Ollama :

1. Fork le projet
2. CrÃ©er une branche feature
3. Modifier les services dans `backend/services/ai/`
4. Ajouter les tests appropriÃ©s
5. Soumettre une Pull Request

## Licence

Ce service fait partie de DocuCortex et suit la mÃªme licence.