# Guide d'utilisation : Service Ollama pour DocuCortex

## üéØ Objectif

Ce guide explique comment utiliser le service d'int√©gration Ollama dans DocuCortex pour b√©n√©ficier des capacit√©s de l'IA locale avec le mod√®le Llama 3.2 3B.

## üìã Table des mati√®res

1. [Installation](#installation)
2. [D√©marrage](#d√©marrage)
3. [Utilisation](#utilisation)
4. [API Endpoints](#api-endpoints)
5. [D√©pannage](#d√©pannage)

## üöÄ Installation

### Option 1 : Installation Automatique (Recommand√©e)

```bash
# Cloner ou naviguer vers le r√©pertoire DocuCortex
cd docucortex_corrige

# Installer automatiquement Ollama et le mod√®le
npm run install:ollama
```

### Option 2 : Installation Manuelle

1. **Installer Ollama**
   ```bash
   # Linux/macOS
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Windows : T√©l√©charger depuis https://ollama.ai
   ```

2. **D√©marrer Ollama**
   ```bash
   ollama serve
   ```

3. **Installer le mod√®le Llama 3.2 3B**
   ```bash
   ollama pull llama3.2:3b
   ```

4. **V√©rifier l'installation**
   ```bash
   npm run ollama:status
   ```

## üèÅ D√©marrage

### D√©marrage avec Support Ollama

```bash
# D√©marrage automatique avec d√©tection Ollama
npm run start:ollama
```

Ce script va :
- ‚úÖ V√©rifier l'environnement
- ‚úÖ D√©tecter Ollama
- ‚úÖ Configurer DocuCortex
- ‚úÖ D√©marrer le serveur

### D√©marrage Standard

```bash
# D√©marrage normal (Ollama d√©tect√© automatiquement)
npm run dev
```

## üìö Utilisation

### 1. Chat Intelligent

**Interface Web**
- Ouvrez DocuCortex dans votre navigateur
- Acc√©dez √† la section Chat IA
- Les r√©ponses seront g√©n√©r√©es par Ollama si disponible

**API Directe**
```bash
# Chat avec Ollama
curl -X POST http://localhost:3000/api/ai/chat/enhanced \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Explique-moi DocuCortex",
    "sessionId": "demo-session",
    "aiProvider": "ollama"
  }'
```

### 2. Analyse de Documents

**R√©sum√© automatique**
```bash
curl -X POST http://localhost:3000/api/ai/ollama/summarize \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Votre texte ici...",
    "maxLength": 200
  }'
```

**Extraction de mots-cl√©s**
```bash
curl -X POST http://localhost:3000/api/ai/ollama/keywords \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Votre texte ici...",
    "maxKeywords": 10
  }'
```

**Analyse de sentiment**
```bash
curl -X POST http://localhost:3000/api/ai/ollama/sentiment \
  -H "Content-Type: application/json" \
  -d '{"text": "Je suis tr√®s satisfait de ce produit"}'
```

### 3. Questions-R√©ponses

**Q&A sur document**
```bash
curl -X POST http://localhost:3000/api/ai/ollama/qa \
  -H "Content-Type: application/json" \
  -d '{
    "documentId": 123,
    "question": "Quel est le prix mentionn√© ?"
  }'
```

### 4. Traduction

```bash
curl -X POST http://localhost:3000/api/ai/ollama/translate \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, how are you?",
    "targetLanguage": "fran√ßais"
  }'
```

## üîå API Endpoints

### Status et Configuration

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/ai/ollama/status` | GET | Statut du service Ollama |
| `/api/ai/ollama/models` | GET | Liste des mod√®les disponibles |
| `/api/ai/ollama/test` | GET | Test de connexion |
| `/api/ai/ollama/stats` | GET | Statistiques d'utilisation |

### Chat et Conversation

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/ai/chat/enhanced` | POST | Chat avec support Ollama |
| `/api/ai/ollama/chat` | POST | Chat direct Ollama |

### Analyse de Contenu

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/ai/ollama/sentiment` | POST | Analyse de sentiment |
| `/api/ai/ollama/summarize` | POST | R√©sum√© de texte |
| `/api/ai/ollama/keywords` | POST | Extraction de mots-cl√©s |
| `/api/ai/ollama/translate` | POST | Traduction |

### Questions-R√©ponses

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/ai/ollama/qa` | POST | Q&A sur document |

### Administration

| Endpoint | M√©thode | Description |
|----------|---------|-------------|
| `/api/ai/ollama/model` | POST | Changer de mod√®le |
| `/api/ai/ollama/reset-stats` | POST | Reset des statistiques |

## üí° Exemples Pratiques

### Exemple 1 : Chat Contextuel

```javascript
// JavaScript - Chat avec historique
const response = await fetch('/api/ai/chat/enhanced', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: "Peux-tu me r√©sumer le document sur les ventes ?",
    sessionId: "user-session-123",
    aiProvider: "ollama"
  })
});

const result = await response.json();
console.log('R√©ponse:', result.response);
console.log('Confiance:', result.confidence);
console.log('Fournisseur IA:', result.aiProvider);
```

### Exemple 2 : Analyse de Sentiment en Lot

```javascript
// JavaScript - Analyse multiple
const texts = [
  "Ce produit est excellent !",
  "Je ne suis pas satisfait",
  "Service correct, pourrait √™tre am√©lior√©"
];

for (const text of texts) {
  const response = await fetch('/api/ai/ollama/sentiment', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text })
  });
  
  const result = await response.json();
  console.log(`"${text}" ‚Üí ${result.sentiment} (${result.confidence})`);
}
```

### Exemple 3 : Workflow Document Complet

```javascript
// JavaScript - Traitement complet d'un document
async function processDocument(documentId, content) {
  // 1. R√©sum√©
  const summary = await fetch('/api/ai/ollama/summarize', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: content, maxLength: 150 })
  });
  
  // 2. Mots-cl√©s
  const keywords = await fetch('/api/ai/ollama/keywords', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: content, maxKeywords: 8 })
  });
  
  // 3. Sentiment
  const sentiment = await fetch('/api/ai/ollama/sentiment', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({ text: content.substring(0, 500) })
  });
  
  return {
    summary: (await summary.json()).summary,
    keywords: (await keywords.json()).keywords,
    sentiment: (await sentiment.json()).sentiment
  };
}
```

## üîç D√©pannage

### Probl√®mes Courants

**1. Ollama non d√©tect√©**
```bash
# V√©rifier l'installation
ollama --version

# R√©installer si n√©cessaire
npm run install:ollama
```

**2. Ollama ne r√©pond pas**
```bash
# V√©rifier le statut
npm run ollama:status

# Red√©marrer Ollama
npm run ollama:stop
ollama serve

# Tester la connexion
npm run ollama:test
```

**3. Mod√®le non disponible**
```bash
# T√©l√©charger le mod√®le
ollama pull llama3.2:3b

# V√©rifier les mod√®les
ollama list
```

**4. Erreur de port**
```bash
# V√©rifier les processus sur le port 11434
lsof -i :11434

# Sur Windows
netstat -ano | findstr :11434
```

### Logs et Debug

**Activer les logs d√©taill√©s**
```bash
DEBUG=* npm run start:ollama
```

**Logs Ollama**
```bash
npm run ollama:logs
```

**Test de connectivit√©**
```bash
# Test Ollama direct
curl http://localhost:11434/api/tags

# Test via DocuCortex
curl http://localhost:3000/api/ai/ollama/test
```

### Performance

**Optimisation**
- Ollama n√©cessite au moins 4GB RAM
- Le mod√®le Llama 3.2 3B utilise ~2GB
- SSD recommand√© pour de meilleures performances

**Monitoring**
```bash
# Statistiques DocuCortex
curl http://localhost:3000/api/ai/ollama/stats

# Statistiques syst√®me (Linux/macOS)
top -p $(pgrep ollama)
```

## üìä Monitoring

### Interface Web

Acc√©dez √† http://localhost:3000/api/ai/ollama/stats pour voir :
- ‚úÖ Nombre de requ√™tes
- ‚úÖ Taux de succ√®s
- ‚úÖ Temps de r√©ponse moyen
- ‚úÖ Utilisation des tokens

### Commandes Utiles

```bash
# Status complet
npm run start:ollama

# Statistiques
curl -s http://localhost:3000/api/ai/ollama/stats | jq

# Mod√®les disponibles
curl -s http://localhost:11434/api/tags | jq '.models[].name'

# Monitoring temps r√©el
watch -n 5 'curl -s http://localhost:3000/api/ai/ollama/stats | jq .stats'
```

## üîí S√©curit√©

- Communication locale uniquement (pas d'envoi vers le cloud)
- Validation des entr√©es utilisateur
- Gestion d'erreurs robuste
- Pas de stockage de donn√©es sensibles

## üéì Conclusion

Le service Ollama offre une alternative potente et priv√©e aux services cloud d'IA. Avec DocuCortex, vous disposez d'un assistant intelligent local capable de :

- ‚úÖ Chat contextuel intelligent
- ‚úÖ Analyse de documents avanc√©e
- ‚úÖ Traitement de texte multilingue
- ‚úÖ Q&A sur vos documents
- ‚úÖ Toutes vos donn√©es restent locales

Pour toute question ou probl√®me, consultez les logs ou utilisez les endpoints de diagnostic fournis.

---

**Version** : 1.0  
**Compatibilit√©** : DocuCortex v3.0+  
**D√©pendances** : Node.js 16+, Ollama, Llama 3.2 3B